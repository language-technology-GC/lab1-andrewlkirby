{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import gzip\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('test.txt', encoding = 'utf-8') as fin, open('tokentest.txt','w', encoding = 'utf-8') as fout:\n",
    "    for line in fin:\n",
    "        tokens = word_tokenize(line)\n",
    "        print(' '.join(tokens), end='\\n', file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: word2vec.py [-h] [--iter ITER] [--min_count MIN_COUNT] --results_path\n",
      "                   RESULTS_PATH --pairs_path PAIRS_PATH [--size SIZE]\n",
      "                   --tok_path TOK_PATH [--window WINDOW]\n",
      "\n",
      "Computes Word2Vec embeddings for a list of target words.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --iter ITER           number of iterations (default: 3)\n",
      "  --min_count MIN_COUNT\n",
      "                        min count\n",
      "  --results_path RESULTS_PATH\n",
      "                        path to output three-column TSV file containing PPMI\n",
      "                        results\n",
      "  --pairs_path PAIRS_PATH\n",
      "                        path to input two-column TSV file containing the\n",
      "                        target word pairs\n",
      "  --size SIZE           embedding size (default: 100)\n",
      "  --tok_path TOK_PATH   path to input tokenized text file\n",
      "  --window WINDOW       symmetric window size (default: 5)\n"
     ]
    }
   ],
   "source": [
    "!python word2vec.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: collecting all words and their counts\n",
      "INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO: collected 16 word types from a corpus of 21 raw words and 4 sentences\n",
      "INFO: Loading a fresh vocabulary\n",
      "INFO: effective_min_count=5 retains 0 unique words (0% of original 16, drops 16)\n",
      "INFO: effective_min_count=5 leaves 0 word corpus (0% of original 21, drops 21)\n",
      "INFO: deleting the raw counts dictionary of 16 items\n",
      "INFO: sample=0.001 downsamples 0 most-common words\n",
      "INFO: downsampling leaves estimated 0 word corpus (0.0% of prior 0)\n",
      "INFO: estimated required memory for 0 words and 100 dimensions: 0 bytes\n",
      "INFO: resetting layer weights\n",
      "Traceback (most recent call last):\n",
      "  File \"word2vec.py\", line 65, in <module>\n",
      "    main(parser.parse_args())\n",
      "  File \"word2vec.py\", line 20, in main\n",
      "    workers=4,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 783, in __init__\n",
      "    fast_version=FAST_VERSION)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 763, in __init__\n",
      "    end_alpha=self.min_alpha, compute_loss=compute_loss)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 910, in train\n",
      "    queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 1081, in train\n",
      "    **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 536, in train\n",
      "    total_words=total_words, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 1187, in _check_training_sanity\n",
      "    raise RuntimeError(\"you must first build vocabulary before training the model\")\n",
      "RuntimeError: you must first build vocabulary before training the model\n"
     ]
    }
   ],
   "source": [
    "!python word2vec.py --results_path resultsw2v.tsv --pairs_path ws353_twocol.tsv --tok_path tokentest.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ppmi.py [-h] --results_path RESULTS_PATH --pairs_path PAIRS_PATH\n",
      "               [--pmi] --tok_path TOK_PATH [--window WINDOW]\n",
      "\n",
      "Computes (P)PMI table for a list of target word pairs.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --results_path RESULTS_PATH\n",
      "                        path to output three-column TSV file containing PPMI\n",
      "                        results\n",
      "  --pairs_path PAIRS_PATH\n",
      "                        path to input two-column TSV file containing the\n",
      "                        target word pairs\n",
      "  --pmi                 compute PMI instead of PPMI\n",
      "  --tok_path TOK_PATH   path to input tokenized text file\n",
      "  --window WINDOW       symmetric window size (default: 10)\n"
     ]
    }
   ],
   "source": [
    "!python ppmi.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 483 words tracked\n",
      "INFO: 204 pairs tracked\n",
      "INFO: 21 tokens counted\n",
      "INFO: 0 pairs covered\n"
     ]
    }
   ],
   "source": [
    "!python ppmi.py --results_path results.tsv --pairs_path ws353_twocol.tsv --tok_path tokentest.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
